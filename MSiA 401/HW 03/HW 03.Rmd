---
title: "HW 03"
author: "Group 10"
date: "2022-10-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 1(a)

```{r}
auto = read.csv('Auto.csv', na.strings = 'NA')
fit_1 = lm(mpg ~ cylinders + displacement + 
             weight + origin + year + 
             I(year^2), data=auto)
summary(fit_1)
```

Positive relationship: displacement, origin, year^2 <br />
Negative relationship: cylinders, weight, year <br />
Weight, origin, year, and year squared are significantly different from 0
$R^2 = 0.8273$

#### 1(b)

```{r}
library(car)
vif(fit_1)
```

Except origin, other variables indicates substantial and even severe multidisciplinary.

#### 1(c)

```{r}
fit_2 = lm(mpg ~ cylinders + displacement + 
             origin + year + I(year^2), 
           data=auto)
summary(fit_2)
```

The parameter estimates' absolute value increase, which indicates the variables exert more fundamental influence on mpg.

$R^2 = 0.7757: \ decreases$

#### 1(d)

```{r}
fit_3 = lm(mpg ~ cylinders + origin + year + 
             I(year^2), data=auto)
summary(fit_3)
```

Compared with model fit_1, the parameter estimates' absolute value increase substantially and R^2 decreases, compared with model fit_2, cylinders and origin increase their estimate parameters' absolute value, whereas estimate parameters of year and year^2 decrease. And R^2 decreases.

#### 2(a)

```{r}
set.seed(1)
x1 = runif(100)
x2 = .5*x1 + rnorm(100)/10
y = 2 + 2*x1 + .3*x2 + rnorm(100)
```

$$
y = 2 + \beta_1x_1 + \beta_2x_2 + \epsilon
\\
Where:
\\
\beta_1 = 2, \
\beta_2 = 0.3, \ 
\epsilon \sim N(0, 1)
$$

#### 2(b)

The correlation between x1 and x2 is `r cor(x1, x2)`.

#### 2(c)

```{r}
fit_4 = lm(y ~ x1 + x2)
summary(fit_4)
confint(fit_4)
```

The parameter estimates for x1 and x2 are 1.4396 and 1.0097. Only x1 is significant at an $\alpha=0.05$

Both true parameters are covered by the confidence intervals of the slope estimates. 

#### 2(d)

```{r}
fit_5 = lm(y ~ x1)
summary(fit_5)
confint(fit_5)
```

$\beta_1$ significantly different from 0. The true $\beta_1$ is still covered by the confidence interval.

#### 2(e)

```{r}
fit_6 = lm(y ~ x2)
summary(fit_6)
confint(fit_6)
```

$\beta_2$ is significantly different than 0. The true $\beta_2$ is not covered by the confidence interval however.

#### 2(f)

Yes, in 2c both confidence intervals are covering the true values. The betas are also both significant. However, in 2e we found that $\beta_2$ alone is not significant. This is weird because in one model it is and one model it isn't. Normally, one would expect a significant independent variable to stay significant no matter what other variables are present in the model or not. This is unless there is multicollinearity. 

#### 3(a)

This equation represents a fork

#### 3(b)

```{r}
set.seed(1)
w = runif(500, min = 0, max = 5)
d = rnorm(500)
e = rnorm(500)
x = w + d
y = 4 + 2*x - 3*w + e

df <- data.frame(y, x, w)

cor(df)
summary(df)
print("Standard Deviation Below (y, x, w)")
print(c(sd(y), sd(x), sd(w)))
```

#### 3(c)

```{r}
fit_3c = lm(y ~ x)
summary(fit_3c)
confint(fit_3c)
```

The 95% CI does not cover the true slope for x and the slope for x is not significant.

#### 3(d)

```{r}
fit_3d = lm(y~x+w)
summary(fit_3d)
confint(fit_3d)
```

The coefficient of x significant at 0.05 level. the 95% CI covers the true slope for x.

#### 3(e)

```{r}
library(car)
vif(fit_3d)
```
The vif for x and w is 2.740063.

#### 4(a)

This equation represents a collider.

#### 4(b)

```{r}
set.seed(1)
x = runif(500, min=0, max=5)
delta = rnorm(500)
e = rnorm(500)
y = x + delta
w = 4 + 2*x + 3*y + e

df = data.frame(y, x, w)
cor(df)
summary(df)
print("Standard Deviation Below (y, x, w)")
print(c(sd(y), sd(x), sd(w)))
```

#### 4(c)

```{r}
fit_4c = lm(y ~ x)
summary(fit_4c)
confint(fit_4c)
```

The coefficient of x is significant at the 0.05 level. 

The 95% CI covers the true slope for x.

#### 4(d)

```{r}
fit_4d = lm(y ~ x + w)
summary(fit_4d)
confint(fit_4d)
```

The coefficient of x is significant at the 0.05 level.

The 95% CI does not cover the true slope for x.

#### 4(e)

```{r}
vif(fit_4d)
```

#### 4(f)

The value of R^2 in the first model is 0.635, the value of R^2 in the second model is 0.964. According to R^2, the second model is better. But this may not be the right model because the vif suggests there are high multicollinearity.

#### 5(a)

This is a pipe.

#### 5(b)

```{r}
set.seed(1)
w = runif(500, min = 0, max = 5)
d = rnorm(500)
e = rnorm(500)
w = x + d
y = 2*w + e

df <- data.frame(y, w)
cor(df)
summary(df)
print("Standard Deviation Below (y, w)")
print(c(sd(y), sd(w)))
```

#### 5(c)

```{r}
fit_5c = lm(y ~ x)
summary(fit_5c)
confint(fit_5c)
```

The coefficient of x is significant at $\alpha = 0.05$

#### 5(d)

```{r}
fit_5d = lm(y ~ x + w)
summary(fit_5d)
confint(fit_5d)
```

We can see that x is not significant and w is significant.