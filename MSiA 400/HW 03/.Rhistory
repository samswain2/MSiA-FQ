knitr::opts_chunk$set(echo = TRUE)
nlm()
choose(5, 2)
choose(100, 18)
n_total = 100
p_1 = 1/6
n_1 = 18
choose(n_total, n_1) * (p_1)^(n_1) * (1 - p_1) * (n_total-n_1)
n_total = 100
p_1 = 1/6
n_1 = 18
choose(n_total, n_1) * (p_1)^(n_1) * (1 - p_1)^(n_total-n_1)
18/100
n_total = 100
p_1 = 1/6
n_1 = 18
choose(n_total, n_1) * (p_1)^(n_1) * (1 - p_1)^(n_total-n_1)
n_total = 100
p_1 = 1/6
n_1 = 18
choose(n_total, n_1) * ((p_1)^(n_1)) * ((1 - p_1)^(n_total-n_1))
knitr::opts_chunk$set(echo = TRUE)
read.csv("gradAdmit.csv")
df <- read.csv(file = 'gradAdmit.csv', header = T)
set.seed(400)
n = nrow(df)
# Get 20% for test set
sample = sample.int(n = n, size = floor(.2*n), replace = F)
train_wrong_index = df[-sample,]
test = df[sample,]
train = train_wrong_index
rownames(train) = 1:nrow(train_wrong_index)
test$admit == 1
mean(test$admit == 1)
mean(train$admit == 1)
mean(train$admit == 1)
mean(test$admit == 1)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train,
scale = T,
probability = T,
kernel = best_params[["k"]],
cost = best_params[["c"]],
degree = best_params[["d"]],
gamma = best_params[["g"]],
coef0 = best_params[["c0"]]
)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
library(e1071)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
library(e1071)
library(MLmetrics)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
library(e1071)
library(MLmetrics)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
pred_final <- predict(svm_final,
test,
decision.values = F,
probability = F)
Precision(test$admit, pred_final, positive = 1)
Precision(test$admit, pred_final, positive = 0)
Precision(test$admit, pred_final, positive = 1)
prec = Precision(test$admit, pred_final, positive = 1)
prec = Precision(test$admit, pred_final, positive = 0)
precision = Precision(test$admit, pred_final)
precision = Precision(test$admit, pred_final)
recall = Recall(test$admit, pred_final)
precision = Precision(test$admit, pred_final)
recall = Recall(test$admit, pred_final)
specificity = Specificity(test$admit, pred_final)
sum(train$admit==1)
sum(train$admit==0)
diff_train = sum(train$admit==1)-sum(train$admit==0)
diff_train = sum(train$admit==0)-sum(train$admit==1)
diff_train = sum(train$admit==0)-sum(train$admit==1)
diff_train/abs(sum(train$admit==1))
diff_train/abs(sum(train$admit==1))*100
SMOTE()
library("DMwR")
install.packages("DMwR")
install.packages("cran/DMwR")
install.packages("remotes")
library(remotes)
remotes::install_github("cran/DMwR")
library("DMwR")
SMOTE()
diff_train = sum(train$admit==0)-sum(train$admit==1)
pct_over = diff_train/abs(sum(train$admit==1))*100
library("DMwR")
SMOTE(admit, train, perc.over = pct_over)
View(train)
SMOTE(admit, train, perc.over = pct_over)
library("DMwR")
SMOTE(train$admit, train, perc.over = pct_over)
library("DMwR")
SMOTE(admit ~ ., train, perc.over = pct_over)
library("DMwR")
SMOTE(admit ~ ., train, perc.over = pct_over, perc.under = 0)
View(train)
library("DMwR")
SMOTE(as.factor(admit) ~ ., train, perc.over = pct_over)
library("DMwR")
train$admit = as.factor(train$admit)
SMOTE(admit ~ ., train, perc.over = pct_over)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
View(train_SMOTe)
mean(train_SMOTe$admit == 1)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
train_combined = rbind(train, train_SMOTe)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
train_combined = rbind(train, train_SMOTe)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_combined,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
table(train_combined$admit)
View(train_SMOTe)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
# train_combined = rbind(train, train_SMOTe)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_combined,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
# train_combined = rbind(train, train_SMOTe)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_SMOTe,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
table(train_combined$admit)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
# train_combined = rbind(train, train_SMOTe)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_SMOTe,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
table(train_combined$admit)
table(train_SMOTe$admit)
library("DMwR")
train$admit = as.factor(train$admit)
train_SMOTe = SMOTE(admit ~ ., train, perc.over = pct_over)
table(train_SMOTe$admit)
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_SMOTe,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
# Model
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_SMOTe,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
# Metrics
precision_3d = Precision(test$admit, pred_final)
recall_3d = Recall(test$admit, pred_final)
specificity_3d = Specificity(test$admit, pred_final)
# Model
svm_final <- svm(factor(admit) ~ gre + gpa + rank,
data = train_SMOTe,
scale = T,
probability = T,
kernel = "polynomial",
cost = 1,
degree = 5,
gamma = 1,
coef0 = 1
)
# Preds
pred_final_3d <- predict(svm_final,
test,
decision.values = F,
probability = F)
# Metrics
precision_3d = Precision(test$admit, pred_final_3d)
recall_3d = Recall(test$admit, pred_final_3d)
specificity_3d = Specificity(test$admit, pred_final_3d)
lambdas = c(1, 2, 4)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -log(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
results_real[0]
lambdas = c(1, 2, 4)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -log(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
}
results_real[0]
results_real
results_real[1]
set.seed(400)
lambdas = c(1)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -log(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
}
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
set.seed(400)
lambdas = c(1)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -ln(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
}
set.seed(400)
lambdas = c(1)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -log(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
}
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
set.seed(400)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -ln(x)
set.seed(400)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -log(x)
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
set.seed(400)
lambdas = c(1)
results = c()
results_real = c()
for (i in lambdas){
n = 10^8/i^2
x = runif(n, 0, 1)
y = -log(x)/i
g = sin(y)/i
results = append(results, sum(g)/n)
results_real = append(results_real, (1 / (1+i^2)))
}
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
pi
y
y > 10
sum(y > 10)
sum(y > 10 * pi)
set.seed(1009)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -ln(x)
set.seed(1009)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -log(x)
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
set.seed(1009)
lambda = 1
n = 10^8
x = runif(n,0,1)
y = -log(x)
y10pi = length(y[y>=(10*pi)])
percy10pi = y10pi/n
mean(y>=(10*pi)])
mean(y>=(10*pi))
knitr::opts_chunk$set(echo = TRUE)
sample = rexp(10^6, 1)
set.seed(400)
sample = rexp(10^6, 1)
set.seed(400)
n = 10^6
d = rexp(n, 1)
total = 0
for (i in d) {
total = total + sin(i) * exp(-10*pi)
}
estimate = total/n
estimate
exp(-10*pi)/2
states = expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1)
View(states)
fair_probs = c(rep(1/6, 6))
length(fair_probs)
length(states
length(states)
length(states)
View(states)
length(states)^
length(states)^10
length(states)
length(states)^4
length(states)^2
View(states)
nrow(states)
print(i)
probs = c()
for (i in n) {
print(i)
}
probs = c()
for (i in 1:n) {
print(i)
}
states = expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1)
n_2b = nrow(states)
unfair_probs = c(4/13, 4/13, 2/13, 2/13, 2/13, 2/13)
fair_probs = c(rep(1/6, 6))
probs = c()
for (i in 1:n_2b) {
print(i)
}
probs = c()
for (i in 1:n_2b) {
p_now = 1 * 0.5
if(states[i,1]==0){
curr_p = curr_p*fair_p[1]
}
else{
curr_p = curr_p*unfair_p[1]
}
for(j in 2:6){
if(states[i,j]!=states[i,j-1]){
curr_p = curr_p * 0.25
}
else{
curr_p = curr_p *0.75
}
if(states[i,j]==0){
curr_p = curr_p *fair_p[j]
}
else{
curr_p = curr_p * unfair_p[j]
}
}
probs = c(probs,curr_p)
}
probs = c()
for (i in 1:n_2b) {
p_now = 1 * 0.5
if(states[i,1]==0){
p_now = p_now*fair_probs[1]
}
else{
p_now = p_now*unfair_probs[1]
}
for(j in 2:6){
if(states[i,j]!=states[i,j-1]){
p_now = p_now * 0.25
}
else{
p_now = p_now *0.75
}
if(states[i,j]==0){
p_now = p_now * fair_probs[j]
}
else{
p_now = p_now * unfair_probs[j]
}
}
probs = c(probs,p_now)
}
max_prob = max(probs)
max_prob_idx = which.max(probs)
print('The combination with the maximum likelihood')
print(states[max_prob_idx,])
print(paste0('The maximum likelihood is ',max_prob))
