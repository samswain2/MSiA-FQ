---
title: "HW 01"
author: "Sam"
date: "2022-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

$$
x^{\top}Ax = 
\begin{bmatrix}
x_{1} & x_{2}
\end{bmatrix}
\begin{bmatrix}
a_{11} & a_{21}\\
a_{12} & a_{22}
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
x_{2}
\end{bmatrix} = 
\begin{bmatrix}
(x_1 * a_{11} + x_2 * a_{12}) & (x_1 * a_{21} + x_2 * a_{22})
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
x_{2}
\end{bmatrix}
\\
= 
\begin{bmatrix}
\sum_{i=1}^{2}a_{i1}x_i & \sum_{i=1}^{2}a_{i2}x_i
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
x_{2}
\end{bmatrix} = 
x_1\sum_{i=1}^{2}a_{i1}x_i + x_2\sum_{i=1}^{2}a_{i1}x_i + x_1\sum_{i=1}^{2}a_{i2}x_i + x_2\sum_{i=1}^{2}a_{i2}x_i = 
\\
\sum_{j=1}^{2}x_j\sum_{i=1}^{2}a_{ij}x_i
\\
Consider \ the\  k^{th} \ row \ in \ the \ above \ vector
\\
\frac{\partial x^{\top} Ax}{\partial x} =
\begin{bmatrix}
\frac{\partial x^{\top} Ax}{\partial x_1}\\
\frac{\partial x^{\top} Ax}{\partial x_2}
\end{bmatrix} = 
\frac{\partial x^{\top} Ax}{\partial x_k} = 
\frac{\partial }{\partial x_k}
(x_1\sum_{i=1}^{2}a_{i1}x_i + x_2\sum_{i=1}^{2}a_{i1}x_i + x_1\sum_{i=1}^{2}a_{i2}x_i + x_2\sum_{i=1}^{2}a_{i2}x_i) = 
\\
x_1a_{k1} + \  ... \  +(\sum_{i=1}^{2}a_{ik}x_i + x_ka_{kk}) + \ ... \ + x_2a_{k2} = 
\sum_{j=1}^{2}a_{kj}x_j + \sum_{i=1}^{2}a_{ik}x_i = 
\\
[(k^{th} \ row \ of \ A) + (transpose \ of \ k^{th} \ column \ of \ A)]x = 
\\
\begin{bmatrix}
[(1^{st} \ row \ of \ A) + (transpose \ of \ 1^{st} \ column \ of \ A)]\\
[(2^{nd} \ row \ of \ A) + (transpose \ of \ 2^{nd} \ column \ of \ A)]
\end{bmatrix}x = 
\\
(A + A^{\top})x
\ As \ we \ know \ A \ is \ symmetric \ then
\\ 
A = A^{\top}. \ Thus \\
\frac{\partial x^{\top} Ax}{\partial x} = 2Ax
$$


## Question 4

$$
\frac{1}{S_{xx}}\sum_{i=1}^{n}(x_i-\bar{x})Cov(\bar{y}, y_i) = 0
$$

## Question 5

```{r}
df <- read.csv("Auto.csv", na.strings = TRUE)
df <- transform(df, horsepower =as.integer(horsepower))
```

#### A.
```{r}
simple_reg <- lm(mpg~horsepower, data = df)
x <- summary(simple_reg)
x$coefficients
```

The estimated regression equation is $mpg = -0.158 * horsepower + 39.94$.

#### B.
The slope indicates that as you increase horsepower the mpg drops.

#### D.
Yes, the p-value of $7.032e^{-81}$ indicates that the relationship is very significant.

#### E.
````{r} 
plot(df$horsepower, df$mpg)
```

Based on the plot above, we can see that the variables seem very strongly related.

#### F.
It tells us how well the model fits the data.

#### G.
```{r}
predict(simple_reg, newdata =  data.frame(horsepower = c(98)))
```

The predicted mpg of a car with $98$ hp is $24.46708$.

#### H.
```{r}
predict(simple_reg, newdata =  data.frame(horsepower = c(98)), interval = "prediction", level = .95)
```

The 95% PI is between $14.8094$ and $34.12476$.

#### I.
```{r}
predict(simple_reg, newdata =  data.frame(horsepower = c(98)), interval = "confidence", level = .99)
```

The 99% CI is between $23.81669$ and $25.11747$.

#### J.
```{r}
confint(simple_reg, 'horsepower', level=0.90)
```

The $90%$ CI for the slope is $-0.1684719$ to $-0.1472176$.

#### K.
```{r}
plot(df$horsepower, df$mpg)
abline(simple_reg, col = "blue")
```

The model doesn't seem to fit very well. From about 100 horsepower and up the model seems to predict a lot better compared to horsepower between 0 and 100.

We assumed the data was linear at the beginning. After plotting the data we see that the data is not very linear. This means our model is not very valid.

## Question 6

#### A.
```{r}
plot(df, pch=".") 
```

Based on the scatter plots there seems to be a relationship between mpg and the following; displacement, horsepower, weight, and year. The other factors do not seem to have a relationship with mpg.

```{r}
round(cor(df[, 1:7], use = "pair"), 4)
```

The correlation is $-0.8044$. This means they are very negatively correlated. This tells us that higher displacement means lower mpg.

#### C.
```{r}
fit = lm(mpg~.-name, df)
summary(fit)
```

The null hypothesis is that none of the factors are significant while the alternative hypothesis is at least one is significant.

The p-value we are using for this decision is $< 2.2e^{-16}$. Since this p-value is less than $0.05$ we conclude at least one factor variable is significant.

#### D.
Displacement, weight, year, and origin appear to be significant.

#### E.
It suggests that newer cars have higher mpgs than older cars.

#### F.
It suggests that cars with higher displacement have higher mpg. In other words, it suggests that there is a positive relationship.

This contradicts what we said in 6b. From 6b, we would think the coefficient would be negative. I think our conclusion in 6b is more plausible because the other factors could be disrupting the effect of displacement and causing the coefficient to be positive when mpg is so negatively correlated with displacement. 